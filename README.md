<<<<<<< HEAD
# ðŸš€ Prompt Optimizer

**Optimize your LLM prompts to reduce costs and improve performance**

Prompt Optimizer is a Python package designed to automatically optimize prompts for Large Language Models (LLM), reducing token consumption while maintaining (or even improving) the semantic quality of the output.

## âœ¨ Key Features

- **ðŸ” Semantic Compression**: Removes redundancy while preserving meaning
- **ðŸ’° Cost Reduction**: Minimizes tokens to reduce API costs
- **ðŸŽ¯ Specific Optimizations**: Adapters for GPT, Claude and other models
- **ðŸ“Š Detailed Metrics**: Tracking of tokens, semantic similarity and costs
- **ðŸ”§ Modular Strategies**: Flexible and customizable system
- **ðŸŒ Multilingual**: Support for Italian and English

## ðŸ“¦ Installation

```bash
pip install prompt-optimizer
=======
# MCP Prompt Optimizer

MCP tool to analyze and optimize prompts from the `prompt_engineering` repository, improving accuracy and token usage.

## ðŸš€ Quick Add to Your IDE

<div align="center" style="display: flex; flex-wrap: wrap; justify-content: center; gap: 12px; margin: 24px 0;">

<a href="SETUP.md#cursor" style="text-decoration: none; display: inline-block;">
  <div style="display: inline-flex; align-items: center; gap: 8px; padding: 14px 28px; background: linear-gradient(135deg, rgba(0, 0, 0, 0.85) 0%, rgba(30, 30, 30, 0.9) 100%); backdrop-filter: blur(20px); border-radius: 16px; border: 1.5px solid rgba(255, 255, 255, 0.15); box-shadow: 0 8px 16px rgba(0, 0, 0, 0.2), 0 4px 8px rgba(0, 0, 0, 0.15), inset 0 1px 0 rgba(255, 255, 255, 0.1); transition: all 0.25s cubic-bezier(0.4, 0, 0.2, 1); cursor: pointer; position: relative; overflow: hidden;">
    <span style="font-size: 18px; line-height: 1;">âš¡</span>
    <span style="font-weight: 600; color: #FFFFFF; font-size: 15px; letter-spacing: 0.3px; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.3);">Add to Cursor</span>
  </div>
</a>

<a href="SETUP.md#visual-studio-code" style="text-decoration: none; display: inline-block;">
  <div style="display: inline-flex; align-items: center; gap: 8px; padding: 14px 28px; background: linear-gradient(135deg, rgba(0, 122, 204, 0.9) 0%, rgba(0, 96, 160, 0.95) 100%); backdrop-filter: blur(20px); border-radius: 16px; border: 1.5px solid rgba(100, 181, 246, 0.4); box-shadow: 0 8px 16px rgba(0, 122, 204, 0.3), 0 4px 8px rgba(0, 122, 204, 0.2), inset 0 1px 0 rgba(255, 255, 255, 0.2); transition: all 0.25s cubic-bezier(0.4, 0, 0.2, 1); cursor: pointer; position: relative; overflow: hidden;">
    <span style="font-size: 18px; line-height: 1;">ðŸ’»</span>
    <span style="font-weight: 600; color: #FFFFFF; font-size: 15px; letter-spacing: 0.3px; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.3);">Add to VS Code</span>
  </div>
</a>

<a href="SETUP.md#windsurf" style="text-decoration: none; display: inline-block;">
  <div style="display: inline-flex; align-items: center; gap: 8px; padding: 14px 28px; background: linear-gradient(135deg, rgba(0, 168, 255, 0.9) 0%, rgba(0, 132, 200, 0.95) 100%); backdrop-filter: blur(20px); border-radius: 16px; border: 1.5px solid rgba(100, 210, 255, 0.4); box-shadow: 0 8px 16px rgba(0, 168, 255, 0.3), 0 4px 8px rgba(0, 168, 255, 0.2), inset 0 1px 0 rgba(255, 255, 255, 0.2); transition: all 0.25s cubic-bezier(0.4, 0, 0.2, 1); cursor: pointer; position: relative; overflow: hidden;">
    <span style="font-size: 18px; line-height: 1;">ðŸŒŠ</span>
    <span style="font-weight: 600; color: #FFFFFF; font-size: 15px; letter-spacing: 0.3px; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.3);">Add to Windsurf</span>
  </div>
</a>

</div>

## Quick Start

### Installation

**Option 1: Automated (Recommended)**
```bash
# Linux/macOS
./install.sh

# Windows (PowerShell)
.\install.ps1
```

**Option 2: Manual**
```bash
# Install UV if not already installed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install dependencies
uv sync
```

### IDE Configuration

The tool is ready to use in **Cursor**, **VS Code**, and **Windsurf**. Configuration files are created automatically by the install script with absolute paths for reliability.

<div align="center" style="display: flex; flex-wrap: wrap; justify-content: center; gap: 12px; margin: 24px 0;">

<a href="SETUP.md#cursor" style="text-decoration: none; display: inline-block;">
  <div style="display: inline-flex; align-items: center; gap: 8px; padding: 14px 28px; background: linear-gradient(135deg, rgba(0, 0, 0, 0.85) 0%, rgba(30, 30, 30, 0.9) 100%); backdrop-filter: blur(20px); border-radius: 16px; border: 1.5px solid rgba(255, 255, 255, 0.15); box-shadow: 0 8px 16px rgba(0, 0, 0, 0.2), 0 4px 8px rgba(0, 0, 0, 0.15), inset 0 1px 0 rgba(255, 255, 255, 0.1); transition: all 0.25s cubic-bezier(0.4, 0, 0.2, 1); cursor: pointer; position: relative; overflow: hidden;">
    <span style="font-size: 18px; line-height: 1;">âš¡</span>
    <span style="font-weight: 600; color: #FFFFFF; font-size: 15px; letter-spacing: 0.3px; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.3);">Add to Cursor</span>
  </div>
</a>

<a href="SETUP.md#visual-studio-code" style="text-decoration: none; display: inline-block;">
  <div style="display: inline-flex; align-items: center; gap: 8px; padding: 14px 28px; background: linear-gradient(135deg, rgba(0, 122, 204, 0.9) 0%, rgba(0, 96, 160, 0.95) 100%); backdrop-filter: blur(20px); border-radius: 16px; border: 1.5px solid rgba(100, 181, 246, 0.4); box-shadow: 0 8px 16px rgba(0, 122, 204, 0.3), 0 4px 8px rgba(0, 122, 204, 0.2), inset 0 1px 0 rgba(255, 255, 255, 0.2); transition: all 0.25s cubic-bezier(0.4, 0, 0.2, 1); cursor: pointer; position: relative; overflow: hidden;">
    <span style="font-size: 18px; line-height: 1;">ðŸ’»</span>
    <span style="font-weight: 600; color: #FFFFFF; font-size: 15px; letter-spacing: 0.3px; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.3);">Add to VS Code</span>
  </div>
</a>

<a href="SETUP.md#windsurf" style="text-decoration: none; display: inline-block;">
  <div style="display: inline-flex; align-items: center; gap: 8px; padding: 14px 28px; background: linear-gradient(135deg, rgba(0, 168, 255, 0.9) 0%, rgba(0, 132, 200, 0.95) 100%); backdrop-filter: blur(20px); border-radius: 16px; border: 1.5px solid rgba(100, 210, 255, 0.4); box-shadow: 0 8px 16px rgba(0, 168, 255, 0.3), 0 4px 8px rgba(0, 168, 255, 0.2), inset 0 1px 0 rgba(255, 255, 255, 0.2); transition: all 0.25s cubic-bezier(0.4, 0, 0.2, 1); cursor: pointer; position: relative; overflow: hidden;">
    <span style="font-size: 18px; line-height: 1;">ðŸŒŠ</span>
    <span style="font-weight: 600; color: #FFFFFF; font-size: 15px; letter-spacing: 0.3px; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.3);">Add to Windsurf</span>
  </div>
</a>

</div>

**For detailed setup instructions, see [SETUP.md](SETUP.md)**

#### Quick Configuration Options

- **Project-level** (default): Configuration files are created in `.cursor/`, `.vscode/`, or `.windsurf/` directories
- **Global Cursor config**: Run `./install.sh --global` to also configure `~/.cursor/mcp.json` for system-wide use

### Verify Installation

```bash
uv run python scripts/verify.py
```

## Features

- **Prompt Analysis**: Token counting, best practices validation, structure analysis
- **Optimization**: Intelligent compression while maintaining quality
- **Cross-Platform Validation**: Consistency across platforms for the same role
- **Test Generation**: Automatic JSON test creation from prompt.toon.md

## Project Structure

```
mcp-prompt-optimizer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ analyzers/      # Analyzers (token, prompt, consistency)
â”‚   â”œâ”€â”€ optimizers/     # Optimizers (token, structure)
â”‚   â”œâ”€â”€ validators/     # Validators (toon, test generator)
â”‚   â”œâ”€â”€ utils.py        # Utilities for repository integration
â”‚   â””â”€â”€ mcp_server.py  # Main MCP server
â”œâ”€â”€ config/             # Configurations and best practices
â”œâ”€â”€ scripts/            # Utility scripts (verify, etc.)
â””â”€â”€ tests/             # Unit tests
```

## Available MCP Tools

### analyze_prompt
Analyze a prompt.toon.md:
- Token usage per block
- Best practices score (0-100)
- Toon structure validation
- Recommendations

### optimize_prompt
Optimize a prompt:
- Token reduction (target: 15-30%)
- Structure improvement
- Best practices application
- Generate optimized version

### validate_consistency
Validate cross-platform consistency:
- Identify core vs specific competencies
- Calculate consistency score
- Detect gaps between platforms
- Suggest normalizations

### token_analysis
Detailed token analysis:
- Breakdown per block
- Cost estimate per execution
- Comparison with platform limits

### generate_tests
Automatically generate JSON tests:
- Baseline/edge-case/compliance scenarios
- Assertions based on output schema
- Repository-compliant templates

### compare_prompts
Compare two prompts:
- Token usage difference
- Efficiency analysis
- Improvement identification

### import_readme
Import and analyze README from prompt_engineering repository:
- Extract platform information
- Extract best practices
- Extract toon format info
- Get repository structure

### compare_json_vs_toon
Compare JSON vs TOON representation for a prompt:
- Show token savings using TOON format
- Compare file sizes
- Estimate format efficiency

## Usage

### Via MCP Server (Recommended)

The tool is exposed as an MCP server. Once configured in your IDE, you can use all the tools directly.

**Start the server manually (for testing):**
```bash
uv run python -m src.mcp_server
# or
uv run mcp-prompt-optimizer
```

### Via Python API

See `EXAMPLE.md` for examples of direct Python class usage.

### IDE Configuration

<div align="center" style="display: flex; flex-wrap: wrap; justify-content: center; gap: 12px; margin: 24px 0;">

<a href="SETUP.md#cursor" style="text-decoration: none; display: inline-block;">
  <div style="display: inline-flex; align-items: center; gap: 8px; padding: 14px 28px; background: linear-gradient(135deg, rgba(0, 0, 0, 0.85) 0%, rgba(30, 30, 30, 0.9) 100%); backdrop-filter: blur(20px); border-radius: 16px; border: 1.5px solid rgba(255, 255, 255, 0.15); box-shadow: 0 8px 16px rgba(0, 0, 0, 0.2), 0 4px 8px rgba(0, 0, 0, 0.15), inset 0 1px 0 rgba(255, 255, 255, 0.1); transition: all 0.25s cubic-bezier(0.4, 0, 0.2, 1); cursor: pointer; position: relative; overflow: hidden;">
    <span style="font-size: 18px; line-height: 1;">âš¡</span>
    <span style="font-weight: 600; color: #FFFFFF; font-size: 15px; letter-spacing: 0.3px; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.3);">Add to Cursor</span>
  </div>
</a>

<a href="SETUP.md#visual-studio-code" style="text-decoration: none; display: inline-block;">
  <div style="display: inline-flex; align-items: center; gap: 8px; padding: 14px 28px; background: linear-gradient(135deg, rgba(0, 122, 204, 0.9) 0%, rgba(0, 96, 160, 0.95) 100%); backdrop-filter: blur(20px); border-radius: 16px; border: 1.5px solid rgba(100, 181, 246, 0.4); box-shadow: 0 8px 16px rgba(0, 122, 204, 0.3), 0 4px 8px rgba(0, 122, 204, 0.2), inset 0 1px 0 rgba(255, 255, 255, 0.2); transition: all 0.25s cubic-bezier(0.4, 0, 0.2, 1); cursor: pointer; position: relative; overflow: hidden;">
    <span style="font-size: 18px; line-height: 1;">ðŸ’»</span>
    <span style="font-weight: 600; color: #FFFFFF; font-size: 15px; letter-spacing: 0.3px; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.3);">Add to VS Code</span>
  </div>
</a>

<a href="SETUP.md#windsurf" style="text-decoration: none; display: inline-block;">
  <div style="display: inline-flex; align-items: center; gap: 8px; padding: 14px 28px; background: linear-gradient(135deg, rgba(0, 168, 255, 0.9) 0%, rgba(0, 132, 200, 0.95) 100%); backdrop-filter: blur(20px); border-radius: 16px; border: 1.5px solid rgba(100, 210, 255, 0.4); box-shadow: 0 8px 16px rgba(0, 168, 255, 0.3), 0 4px 8px rgba(0, 168, 255, 0.2), inset 0 1px 0 rgba(255, 255, 255, 0.2); transition: all 0.25s cubic-bezier(0.4, 0, 0.2, 1); cursor: pointer; position: relative; overflow: hidden;">
    <span style="font-size: 18px; line-height: 1;">ðŸŒŠ</span>
    <span style="font-weight: 600; color: #FFFFFF; font-size: 15px; letter-spacing: 0.3px; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.3);">Add to Windsurf</span>
  </div>
</a>

</div>

- **Cursor**: See [SETUP.md](SETUP.md#cursor) for Cursor configuration
- **VS Code**: See [SETUP.md](SETUP.md#visual-studio-code) for VS Code configuration  
- **Windsurf**: See [SETUP.md](SETUP.md#windsurf) for Windsurf configuration

### Import README

To import the README from the prompt_engineering repository:

```python
from src.analyzers.readme_importer import ReadmeImporter

importer = ReadmeImporter()
context = importer.import_to_context()

# Access:
# - context['readme_content']: full content
# - context['platforms']: list of platforms with roles
# - context['best_practices']: extracted best practices
# - context['toon_format']: toon format info
# - context['repository_structure']: repository structure
```

## Installation Methods

### Local Installation (Recommended)

Install in the project directory:
```bash
uv sync
```

This creates a virtual environment and installs dependencies locally.

### Global Installation

Install as a global package:
```bash
uv pip install -e .
```

Then use from anywhere:
```bash
mcp-prompt-optimizer
>>>>>>> 6f7db4b (feat: implement 100% plug-and-play setup with Material Design buttons)
```

### Development Installation

<<<<<<< HEAD
```bash
git clone https://github.com/yourusername/prompt-optimizer.git
cd prompt-optimizer
pip install -e ".[dev]"
```

## ðŸš€ Quick Start

### Esempio Base

```python
from prompt_optimizer import PromptOptimizer
from prompt_optimizer.adapters import OpenAIAdapter
from prompt_optimizer.strategies import (
    SemanticCompressionStrategy,
    TokenReductionStrategy,
)

# Initialize optimizer with GPT adapter
optimizer = PromptOptimizer(
    llm_adapter=OpenAIAdapter("gpt-4"),
    strategies=[
        SemanticCompressionStrategy(),
        TokenReductionStrategy(),
    ]
)

# Original prompt
original_prompt = """
Please could you very kindly analyze the following text and provide 
a very detailed explanation of the main concepts. It is very important 
that you explain everything in a clear way. Thank you very much for 
your help with this task.
"""

# Optimize
result = optimizer.optimize(original_prompt)

print(f"Original prompt: {result.original_prompt}")
print(f"Optimized prompt: {result.optimized_prompt}")
print(f"Tokens saved: {result.token_reduction}")
print(f"Cost reduction: ${result.cost_reduction:.4f}")
print(f"Semantic similarity: {result.semantic_similarity:.2%}")
```

### Optimization with Specific Model

```python
from prompt_optimizer import PromptOptimizer
from prompt_optimizer.adapters import ClaudeAdapter
from prompt_optimizer.strategies import StructuralOptimizationStrategy

# Configuration for Claude
adapter = ClaudeAdapter("claude-3-sonnet")
optimizer = PromptOptimizer(
    llm_adapter=adapter,
    strategies=[StructuralOptimizationStrategy()],
    aggressive_mode=False,
    preserve_meaning_threshold=0.90
)

# Optimize complex prompt
complex_prompt = """
Context: We are analyzing customer feedback.
Task: Please analyze the sentiment.
Also, identify key themes.
And provide recommendations.
Example: "Great product" = positive sentiment.
"""

result = optimizer.optimize(complex_prompt)

# Display statistics
print(f"Original tokens: {result.metadata['original_tokens']}")
print(f"Optimized tokens: {result.metadata['optimized_tokens']}")
print(f"Reduction percentage: {result.metadata['reduction_percentage']:.1%}")
```

## ðŸ“š Optimization Strategies

### 1. Semantic Compression Strategy

Removes semantic redundancy, filler words and simplifies verbose constructs.

```python
from prompt_optimizer.strategies import SemanticCompressionStrategy

strategy = SemanticCompressionStrategy()
optimized = strategy.apply(
    "I would like to very kindly ask you to please analyze this text"
)
# Output: "Analyze this text"
```

**Techniques applied:**
- Removal of filler words (very, really, actually, etc.)
- Elimination of redundant phrases
- Simplification of complex constructs
- Condensation of equivalent sentences

### 2. Token Reduction Strategy

Reduces the number of tokens through abbreviations, contractions and symbolization.

```python
from prompt_optimizer.strategies import TokenReductionStrategy

strategy = TokenReductionStrategy()
optimized = strategy.apply(
    "The maximum information about the configuration is needed"
)
# Output: "Max info about config needed"
```

**Techniques applied:**
- Standard abbreviations (information â†’ info)
- Grammatical contractions (do not â†’ don't)
- Symbolization (and â†’ &, at â†’ @)
- Optimization of numbers and dates

### 3. Structural Optimization Strategy

Reorganizes the prompt to maximize effectiveness with LLM models.

```python
from prompt_optimizer.strategies import StructuralOptimizationStrategy

strategy = StructuralOptimizationStrategy()
optimized = strategy.apply(messy_prompt)
# Output: Well-structured prompt with clear sections
```

**Techniques applied:**
- Reorganization into logical sections
- Optimization of instruction order
- Consolidation of duplicate instructions
- LLM-friendly formatting

## ðŸŽ¯ Model Adapters

### OpenAI GPT

```python
from prompt_optimizer.adapters import OpenAIAdapter

# Supports: gpt-3.5-turbo, gpt-4, gpt-4-turbo, gpt-4o
adapter = OpenAIAdapter("gpt-4")

# Count tokens accurately (uses tiktoken if available)
token_count = adapter.count_tokens("Your prompt")

# Calculate costs
cost = adapter.calculate_cost(input_tokens=1000, output_tokens=500)

# GPT-specific optimizations
optimized = adapter.optimize_for_model(prompt)

# Suggestions
suggestions = adapter.suggest_optimizations(prompt)
```

### Anthropic Claude

```python
from prompt_optimizer.adapters import ClaudeAdapter

# Supports: claude-2, claude-3-haiku, claude-3-sonnet, claude-3-opus
adapter = ClaudeAdapter("claude-3-sonnet")

# Token count for Claude
token_count = adapter.count_tokens("Your prompt")

# Claude-specific optimizations (XML tags, etc.)
optimized = adapter.optimize_for_model(prompt)

# Check context capacity
can_fit = adapter.can_fit_in_context(prompt, reserve_tokens=1000)
```

## ðŸ“Š Metrics and Analysis

### Token Analysis

```python
from prompt_optimizer.metrics import TokenMetrics

metrics = TokenMetrics()

# Detailed analysis
analysis = metrics.analyze_tokens(prompt)
print(f"Total tokens: {analysis.total_tokens}")
print(f"Unique tokens: {analysis.unique_tokens}")
print(f"Redundancy: {analysis.redundancy_score:.2%}")

# Reduction potential
potential = metrics.calculate_reduction_potential(prompt)
print(f"Estimated reduction: {potential:.1%}")
```

### Semantic Analysis

```python
from prompt_optimizer.metrics import SemanticMetrics

metrics = SemanticMetrics()

# Similarity between texts
similarity = metrics.calculate_similarity(original, optimized)
print(f"Semantic similarity: {similarity:.2%}")

# Content analysis
analysis = metrics.analyze_semantic_content(prompt)
print(f"Semantic density: {analysis.semantic_density:.2f}")
print(f"Complexity: {analysis.complexity_score:.2f}")
print(f"Key concepts: {analysis.key_concepts}")
```

## ðŸ”§ Advanced Configuration

### Strategy Customization

```python
from prompt_optimizer.strategies import OptimizationConfig

config = OptimizationConfig(
    aggressive_mode=True,
    preserve_structure=False,
    target_reduction=0.30,  # Target 30% reduction
    custom_params={
        'use_xml_tags': True,  # For Claude
        'encourage_reasoning': True
    }
)

strategy = SemanticCompressionStrategy(config=config)
```

### Batch Optimization

```python
prompts = [
    "First prompt to optimize...",
    "Second prompt...",
    "Third prompt..."
]

# Optimize all prompts
results = optimizer.batch_optimize(
    prompts,
    target_reduction=0.25,
    preserve_structure=True
)

for i, result in enumerate(results):
    print(f"Prompt {i+1}: {result.token_reduction} tokens saved")
```

### Custom Thresholds

```python
optimizer = PromptOptimizer(
    llm_adapter=adapter,
    strategies=strategies,
    preserve_meaning_threshold=0.95,  # Maximum meaning preservation
    aggressive_mode=False  # Conservative optimization
)
```

## ðŸ’¡ Practical Examples

### Example 1: Cost Reduction Optimization

```python
from prompt_optimizer import PromptOptimizer
from prompt_optimizer.adapters import OpenAIAdapter
from prompt_optimizer.strategies import TokenReductionStrategy

# Configure for maximum cost reduction
adapter = OpenAIAdapter("gpt-4")  # Expensive model
optimizer = PromptOptimizer(
    llm_adapter=adapter,
    strategies=[TokenReductionStrategy()],
    aggressive_mode=True
)

verbose_prompt = """
I would very much appreciate it if you could please take the time to 
carefully analyze the following document and provide a comprehensive 
summary of all the main points that are discussed throughout the text.
"""

result = optimizer.optimize(verbose_prompt)
print(f"Estimated savings: ${result.cost_reduction:.4f}")
```

### Example 2: Structure-Preserving Optimization

```python
from prompt_optimizer import PromptOptimizer
from prompt_optimizer.strategies import StructuralOptimizationStrategy

optimizer = PromptOptimizer(
    strategies=[StructuralOptimizationStrategy()]
)

structured_prompt = """
Analyze this data. Consider trends. Look at patterns. 
Identify anomalies. Provide insights. Make recommendations.
"""

result = optimizer.optimize(
    structured_prompt,
    preserve_structure=True
)
# Output: Prompt reorganized into logical sections
```

### Example 3: Pre-Optimization Analysis

```python
from prompt_optimizer.adapters import ClaudeAdapter

adapter = ClaudeAdapter("claude-3-opus")

# Analyze prompt before optimizing
suggestions = adapter.suggest_optimizations(your_prompt)

print("Prompt analysis:")
print(f"Current tokens: {suggestions['current_tokens']}")
print(f"Context usage: {suggestions['context_usage_percent']:.1f}%")
print(f"Estimated cost: ${suggestions['estimated_cost']:.4f}")

print("\nSuggestions:")
for suggestion in suggestions['suggestions']:
    print(f"- [{suggestion['severity']}] {suggestion['message']}")
```

## ðŸ§ª Testing

```bash
# Run all tests
pytest

# Tests with coverage
pytest --cov=prompt_optimizer --cov-report=html

# Specific tests
pytest tests/test_strategies.py
pytest tests/test_adapters.py
```

## ðŸ› ï¸ Development

### Environment Setup

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\\Scripts\\activate  # Windows

# Install dev dependencies
pip install -e ".[dev]"

# Setup pre-commit hooks
pre-commit install
```

### Code Quality

```bash
# Format code
black src/

# Sort imports
isort src/

# Lint
flake8 src/

# Type checking
mypy src/
```

## ðŸ“ˆ Performance

Benchmark on dataset of 1000 prompts:

| Strategy | Average Reduction | Average Time | Similarity |
|-----------|----------------|-------------|------------|
| Semantic Compression | 25% | 0.15s | 92% |
| Token Reduction | 18% | 0.08s | 95% |
| Structural | 12% | 0.22s | 98% |
| Combined | 35% | 0.40s | 90% |

## ðŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a branch for your feature (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

## ðŸ“„ License

This project is distributed under the MIT License. See the `LICENSE` file for details.

## ðŸ™ Acknowledgments

- OpenAI for tiktoken
- Anthropic for prompt engineering best practices
- The AI developer community

## ðŸ“ž Contact

- **Email**: your.email@example.com
- **GitHub**: [@yourusername](https://github.com/yourusername)
- **Issues**: [GitHub Issues](https://github.com/yourusername/prompt-optimizer/issues)

## ðŸ—ºï¸ Roadmap

- [ ] Support for more LLM models (Llama, Mistral, etc.)
- [ ] Web interface for testing
- [ ] Optimization results caching
- [ ] ML-based optimization strategies
- [ ] LangChain integration
- [ ] Support for multimodal prompts

---

**Made with â¤ï¸ for the AI community**
=======
For development with dev dependencies:
```bash
uv sync --dev
```

## Supported Best Practices

- Context stacking
- Explicit role + responsibilities
- Verifiable formats (JSON/YAML/Markdown)
- Positive/negative examples
- Citation management
- Modular toon format

## TOON Format Integration

The tool uses [toon-format](https://github.com/toon-format/toon-python) for:
- Optimized parsing of `.toon.md` files
- JSON vs TOON efficiency comparison (30-60% token reduction)
- Support for native TOON format in addition to YAML frontmatter

All parsers use `ToonParser` which supports both YAML frontmatter (standard) and pure TOON format.

## Success Metrics

- **Accuracy**: Best practices score >80%
- **Token Usage**: 15-30% reduction while maintaining quality
- **Consistency**: >90% alignment across platforms for the same role
- **Validation**: Automatic test generation with >95% coverage

## Configuration

### Environment Variables

- `MCP_PROMPT_OPTIMIZER_REPO_PATH`: Path to the `prompt_engineering` repository
  - Default: `../prompt_engineering` (relative to mcp-prompt-optimizer)
  - Can be set in IDE configuration files or system environment

### IDE Configuration Files

Configuration files are automatically created by the install script:
- `.cursor/mcp.json` - Cursor configuration
- `.vscode/settings.json` - VS Code configuration
- `.windsurf/mcp.json` - Windsurf configuration

Example files are available in `config/` directory.

## Troubleshooting

See [SETUP.md](SETUP.md#troubleshooting) for detailed troubleshooting guide.

Common issues:
- MCP server not starting â†’ Check UV installation and dependencies
- Tools not available â†’ Verify IDE configuration and restart IDE
- Repository path issues â†’ Set `MCP_PROMPT_OPTIMIZER_REPO_PATH` environment variable

>>>>>>> 6f7db4b (feat: implement 100% plug-and-play setup with Material Design buttons)
